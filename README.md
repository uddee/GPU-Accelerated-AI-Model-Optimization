# GPU-Accelerated AI Model Optimization

This project demonstrates GPU-accelerated AI model optimization using PyTorch and CUDA. The code showcases how to implement deep learning model acceleration techniques, optimize inference performance, analyze memory usage, and benchmark model performance across different GPU architectures.

## Features
- Utilizes **PyTorch** and **CUDA** to leverage GPU acceleration.
- Implements a simple feed-forward neural network for **image classification** (e.g., MNIST).
- Benchmarks model training time and GPU memory usage.
- Optimizes inference performance on **GPU architectures**.

## Requirements
To run the code, ensure you have the following installed:

- Python 3.x
- PyTorch (with CUDA support)
- CUDA-enabled GPU (optional, for GPU acceleration)
- Jupyter Notebook (optional, for running the code in a notebook)

You can install the required Python libraries using:

```bash
pip install torch torchvision
